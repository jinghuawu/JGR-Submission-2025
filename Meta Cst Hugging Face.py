import os
import pandas as pd
import numpy as np
import joblib
import gradio as gr
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.ensemble import RandomForestRegressor
import torch
from pytorch_tabnet.tab_model import TabNetClassifier

# å…¨å±€é…ç½®
ELEMENT_FEATURES = ['Al', 'Sc', 'Ti', 'V', 'Fe', 'Ga', 'W', 'Sb', 'Zr', 'Hf', 'Nb', 'Ta', 'U']
DERIVED_FEATURES = ['Zr/Hf', 'Nb/Ta', 'Sb/W', 'Fe/Al', 'U/Hf', 'U/Zr']

class MetaCassiterite:
    def __init__(self):
        self.models = {
            "Random Forest": None,
            "XGBoost": None,
            "TabNet": None,
            "Stacking": None
        }
        self.load_models()
        self.processed_data = None
        self.prediction_results = None
        self.sample_ids = None

    def resource_path(self, relative_path):
        """ è·å–èµ„æºçš„ç»å¯¹è·¯å¾„ """
        base_path = os.path.abspath(".")
        return os.path.join(base_path, relative_path)

    def load_models(self):
        """åŠ è½½æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            self.models["Random Forest"] = joblib.load(self.resource_path('best_rf_model.pkl'))
            self.models["XGBoost"] = joblib.load(self.resource_path('best_xgb_model.pkl'))
            self.models["TabNet"] = joblib.load(self.resource_path('best_tabnet_model.pkl'))
            self.models["Ensemble"] = joblib.load(self.resource_path('best_ensem_model.pkl'))
            return True
        except Exception as e:
            print(f"Error loading models: {str(e)}")
            return False

    def apply_log10_transform(self, df):
        """å¯¹å…ƒç´ ç‰¹å¾è¿›è¡ŒLog10å˜æ¢"""
        transformed_df = df.copy()
        for col in ELEMENT_FEATURES:
            if col in transformed_df.columns:
                mask = transformed_df[col].notna() & (transformed_df[col] > 0)
                transformed_df.loc[mask, col] = np.log10(transformed_df.loc[mask, col])
        return transformed_df

    def impute_missing_values(self, df):
        """å¤šé‡æ’è¡¥ç¼ºå¤±å€¼"""
        cols_to_impute = [col for col in ELEMENT_FEATURES if col in df.columns]
        if not cols_to_impute:
            return df

        data_to_impute = df[cols_to_impute].copy()
        if not data_to_impute.isna().any().any():
            return df

        imputed_data_list = []
        for i in range(5):
            estimator = RandomForestRegressor(
                n_estimators=50, max_depth=7,
                n_jobs=-1, random_state=2025+i
            )
            imp = IterativeImputer(
                estimator=estimator, max_iter=15,
                random_state=2025+i, tol=0.01
            )
            imputed_data = imp.fit_transform(data_to_impute)
            imputed_data_list.append(imputed_data)

        imputed_avg = np.mean(imputed_data_list, axis=0)
        imputed_df = df.copy()
        imputed_df[cols_to_impute] = imputed_avg
        return imputed_df

    def create_derived_features(self, df):
        """åˆ›å»ºæ´¾ç”Ÿç‰¹å¾"""
        df_with_derived = df.copy()

        if 'Zr' in df.columns and 'Hf' in df.columns:
            df_with_derived['Zr/Hf'] = df['Zr'] - df['Hf']
        if 'Nb' in df.columns and 'Ta' in df.columns:
            df_with_derived['Nb/Ta'] = df['Nb'] - df['Ta']
        if 'Sb' in df.columns and 'W' in df.columns:
            df_with_derived['Sb/W'] = df['Sb'] - df['W']
        if 'Fe' in df.columns and 'Al' in df.columns:
            df_with_derived['Fe/Al'] = df['Fe'] - df['Al']
        if 'U' in df.columns and 'Hf' in df.columns:
            df_with_derived['U/Hf'] = df['U'] - df['Hf']
        if 'U' in df.columns and 'Zr' in df.columns:
            df_with_derived['U/Zr'] = df['U'] - df['Zr']

        return df_with_derived

    def process_data(self, file_path):
        """å¤„ç†ä¸Šä¼ çš„æ•°æ®"""
        try:
            # è¯»å–æ–‡ä»¶
            if file_path.endswith('.csv'):
                df = pd.read_csv(file_path)
            else:
                df = pd.read_excel(file_path)

            # è¯†åˆ«æ ·æœ¬ID
            possible_id_cols = ['ID', 'Sample_ID', 'SampleID', 'Sample', 'Sample ID', 'id']
            self.sample_id_column = next(
                (col for col in possible_id_cols if col in df.columns), None
            )
            self.sample_ids = df[self.sample_id_column].astype(str).tolist() if self.sample_id_column else [
                f"Sample_{i+1}" for i in range(len(df))]

            # æ•°æ®å¤„ç†æµç¨‹
            log_data = self.apply_log10_transform(df)
            imputed_data = self.impute_missing_values(log_data)
            processed_data = self.create_derived_features(imputed_data)

            # å‡†å¤‡ç‰¹å¾æ•°æ®
            feature_cols = [col for col in ELEMENT_FEATURES + DERIVED_FEATURES if col in processed_data.columns]
            X = processed_data[feature_cols].values
            self.processed_data = X
            self.feature_names = feature_cols

            return processed_data, "\n".join([
                f"Loading Successfully: {os.path.basename(file_path)}",
                f"Sample X {len(df)}",
                f"Features X {len(feature_cols)}"
            ])

        except Exception as e:
            return None, f"Failed to process: {str(e)}"

    def predict(self):
        """æ‰§è¡Œé¢„æµ‹"""
        if self.processed_data is None:
            return None, "Please upload and process data"

        try:
            results = {"Sample_ID": self.sample_ids}
            class_map = {0: 'A', 1: 'B'}  # æ ¹æ®ä½ çš„å®é™…ç±»åˆ«ä¿®æ”¹

            # éšæœºæ£®æ—é¢„æµ‹
            rf_pred = self.models["Random Forest"].predict(self.processed_data)
            rf_proba = self.models["Random Forest"].predict_proba(self.processed_data)
            results["RF_Class"] = [class_map[p] for p in rf_pred]
            results["RF_Prob"] = np.max(rf_proba, axis=1)

            # XGBoosté¢„æµ‹
            xgb_pred = self.models["XGBoost"].predict(self.processed_data)
            xgb_proba = self.models["XGBoost"].predict_proba(self.processed_data)
            results["XGB_Class"] = [class_map[p] for p in xgb_pred]
            results["XGB_Prob"] = np.max(xgb_proba, axis=1)

            # TabNeté¢„æµ‹
            tabnet_pred = self.models["TabNet"].predict(self.processed_data)
            tabnet_proba = self.models["TabNet"].predict_proba(self.processed_data)
            results["TabNet_Class"] = [class_map[p] for p in tabnet_pred]
            results["TabNet_Prob"] = np.max(tabnet_proba, axis=1)

            # é›†æˆé¢„æµ‹
            stack_pred = self.models["Ensemble"].predict(self.processed_data)
            stack_proba = self.models["Ensemble"].predict_proba(self.processed_data)
            results["Ensemble_Class"] = [class_map[p] for p in stack_pred]
            results["Ensemble_Prob"] = np.max(stack_proba, axis=1)

            # åˆ›å»ºç»“æœDataFrame
            self.prediction_results = pd.DataFrame(results)
            return self.prediction_results, "Prediction Finished"

        except Exception as e:
            return None, f"Error in prediciton: {str(e)}"
            
    def export_results(self, format_type="excel"):
        """Export Results (Excel or CSV)"""
        if self.prediction_results is None:
            return None, "No Results for Exportingï¼ŒPlease Finish Prediction"
            
        try:
            timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
            
            if format_type == "excel":
                output_file = f"Prediction_Results_{timestamp}.xlsx"
                self.prediction_results.to_excel(output_file, index=False)
            else:
                output_file = f"Prediction_Results_{timestamp}.csv"
                self.prediction_results.to_csv(output_file, index=False)
                
            return output_file, f"Exported successfully as {output_file}"
        except Exception as e:
            return None, f"Export Error: {str(e)}"

# åˆå§‹åŒ–ç³»ç»Ÿ
system = MetaCassiterite()

# Gradioç•Œé¢
custom_css = """
    .title-text {
        font-size: 36px;
        font-weight: bold;
        text-align: center;
        margin-bottom: 20px;
    }
    .main-button {
        width: 100%;
        padding: 15px;
        font-size: 16px;
        margin: 10px 0;
    }
    #ä¸‰åˆ—æ¨¡å—
    .tab-nav button {
    font-size: 36px !important;
    font-weight: bold !important;
    flex: 1;
    padding: 12px !important;
    color: #9075ad !important; 
}
"""

with gr.Blocks(title="Meta Cassiterite 1.0", css=custom_css) as demo:
    gr.HTML("""
    <div class="title-text">
        ğŸ’ Meta Cassiterite 1.0 ğŸ’
    </div>
    """)

    with gr.Tabs() as tabs:
        with gr.Row():
            with gr.Column(scale=1):
                with gr.TabItem("ğŸ“ Upload & Process Data") as tab1:
                    file_input = gr.File(label="Upload file (CSV/Excel)")
                    process_btn = gr.Button("Process Data", elem_classes=["main-button"])
                    processed_data = gr.Dataframe(label="Processed Data", interactive=False)
                    process_output = gr.Textbox(label="Processing Status")

            with gr.Column(scale=1):
                with gr.TabItem("ğŸ”® Prediction") as tab2:
                    predict_btn = gr.Button("Start Prediction", elem_classes=["main-button"])
                    prediction_results = gr.Dataframe(label="Predicted Results")
                    predict_output = gr.Textbox(label="Prediction Status")

            with gr.Column(scale=1):
                with gr.TabItem("ğŸ’¾ Export") as tab3:
                    with gr.Row():
                        excel_btn = gr.Button("Export Excel File", variant="primary", elem_classes=["main-button"])
                        csv_btn = gr.Button("Export CSV File", elem_classes=["main-button"])
                    export_output = gr.Textbox(label="Exporting Status")
                    export_download = gr.File(label="Download File")

    # äº‹ä»¶å¤„ç†
    process_btn.click(
        fn=system.process_data,
        inputs=file_input,
        outputs=[processed_data, process_output]
    )

    predict_btn.click(
        fn=system.predict,
        outputs=[prediction_results, predict_output]
    )

    excel_btn.click(
        fn=lambda: system.export_results("excel"),
        outputs=[export_download, export_output]
    )
    
    csv_btn.click(
        fn=lambda: system.export_results("csv"),
        outputs=[export_download, export_output]
    )
    
    # Add the notes at the bottom
    gr.Markdown("""
        ## Notes:
        1. Please input element concentrations in order of Al, Sc, Ti, V, Fe, Ga, W, Sb, Zr, Hf, Nb, Ta, and U, and delete BDL and NA characters
        2. LOG10 transformation and imputation of element concentrations will be automatically processed
        3. Missing values will be imputed using Random Forest (average of 5 iterations)
        4. Prediction using multiple models (Random Forest, XGBoost, TabNet, Ensemble)
    """)

if __name__ == "__main__":
    demo.launch()